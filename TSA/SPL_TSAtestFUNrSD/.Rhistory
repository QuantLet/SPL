!= length(unique(s$Date)))) {
stop(paste("For at least one date, there are less than 5",
"or none observations available."))
}
head(df)
# Read data sets
df = fread(file)
head(df)
# Read data sets
df = fread(file)
# Reformat Tweet texts as character and timestamp as date
if("text" %in% colnames(df) & "created_at" %in% colnames(df)) {
df$text       = as.character(df$text)
df$created_at = ymd_hm(df$created_at)
} else {
stop(paste("Data file does not contain columns",
"named 'text' and 'created_at'"))
}
# Get sentiment scores for each tweet
s = data.frame("Date"      = as.Date(df$created_at ,  format = '%Y/%m/%d'),
"Sentiment" = get_sentiment(df$text, method = "syuzhet"))
# Check that for each date present in time series at least five Tweets
# are available and each date between minimum and maximum date is contained
if((min(unique(table(s$Date))) < 5) |
(sum(seq(min(s$Date), max(s$Date), by = 1) %in% s$Date)
!= length(unique(s$Date)))) {
stop(paste("For at least one date, there are less than 5",
"or none observations available."))
}
sentiment <- "positive"
as.vector(by(s, s$Date, function(x) {
sum(x[x$Sentiment > 0, 2])/100
}))
as.vector(by(s, s$Date, function(x) {
sum(x[x$Sentiment > 0, 2]/length(x))
}))
x <- s
as.vector(by(s, s$Date, function(x) {
sum(x[x$Sentiment > 0, 2])/length(x)
}))
as.vector(by(s, s$Date, function(x) {
sum(x[x$Sentiment > 0, 2])/nrow(x)
}))
rm(list = ls())
# Load required packages
if(!require("tseries")) install.packages("tseries")
library("tseries")
# Source custom functions
function.sources = list.files(pattern = "(TSAfun_){1}.*.R")
sapply(function.sources, source, .GlobalEnv)
# Function for easy string pasting
"%&%" = function(x, y) paste(x, y, sep = "")
# Repeat whole analysis for positive as well as negative sentiment share
for(sentiment in c("positive", "negative")){
# Get data
ts =  getData("tweets_raw.csv", sentiment)[[2]]
# Check for missing values
if(any(is.na(ts))) {
stop(paste("There are missing values present in this time series.",
"Interpolate raw data first.", sep = " "))
}
# Test for stationarity
if(adf.test(ts)$p.value > 0.05) {
print(paste("Test for stationarity could not reject H0:",
"Time series has a unit root.",
"Consider detrending your time series.",
sep = " "))
q <- readline("Would you like to proceed anyway? (Y/N)")
if(regexpr(q, 'y', ignore.case = TRUE) == 1){
next
} else {
break
}
}
### Calculate rolling window statistics for ts ###
# Set maximium window size
max.ws = floor(length(ts)/2)
# Lag-1-autocorrelation
acf  = c(1:length(ts))
for(i in 3:max.ws) {
temp = getMovingWindowACF(ts, w = i)
acf  = data.frame(acf, c(rep(NA, i-1), temp))
colnames(acf)[ncol(acf)] = "acf_ws"%&%i
}
# Variance
var  = c(1:length(ts))
for(i in 3:max.ws) {
temp = getMovingWindowVAR(ts, w = i)
var  = data.frame(var, c(rep(NA, i-1), temp))
colnames(var)[ncol(var)] = "var_ws"%&%i
}
# Skewness
skew = c(1:length(ts))
for(i in 3:max.ws) {
temp = getMovingWindowSKEW(ts, w = i)
skew = data.frame(skew, c(rep(NA, i-1), temp))
colnames(skew)[ncol(skew)] = "skew_ws"%&%i
}
# W_2 measure
W2 = c(1:length(ts))
for(ws in 3:max.ws) {
n    = c(rep(NA, (ws-1)), 1:(length(ts)-ws+1))
temp =   (acf[1:nrow(acf), 2] -
c(rep(NA, ws-1), cumsum(acf[ws:nrow(acf) , 2])) / n) /
getRunningSD(acf, ws)
+ (var[1:nrow(var), 2] -
c(rep(NA, ws-1), cumsum(var[ws:nrow(var) , 2])) / n) /
getRunningSD(var, ws)
+ (skew[1:nrow(skew), 2] -
c(rep(NA, ws-1), cumsum(skew[ws:nrow(skew) , 2])) / n) /
getRunningSD(skew, ws)
W2   = data.frame(W2, temp)
colnames(W2)[ncol(W2)] = "W2_ws"%&%ws
}
# Create data frame containing running mean/standard deviation
# of W2 measure to identify CSD areas
flag = flagCSD(W2)
# Write out data frames
#for(i in c("acf", "var", "skew", "flag", "W2", "ts")){
#    write.csv(i, ""%&%sentiment%&%"_"%&%i%&%".csv")
#}
# Tidy up
rm(max.ws, n, temp, ws, i)
}
# end of file
rm(list = ls())
# Load required packages
if(!require("tseries")) install.packages("tseries")
library("tseries")
# Source custom functions
function.sources = list.files(pattern = "(TSAfun_){1}.*.R")
sapply(function.sources, source, .GlobalEnv)
# Function for easy string pasting
"%&%" = function(x, y) paste(x, y, sep = "")
# Repeat whole analysis for positive as well as negative sentiment share
for(sentiment in c("positive", "negative")){
# Get data
ts =  getData("tweets_raw.csv", sentiment)[[2]]
# Check for missing values
if(any(is.na(ts))) {
stop(paste("There are missing values present in this time series.",
"Interpolate raw data first.", sep = " "))
}
# Test for stationarity
if(adf.test(ts)$p.value > 0.05) {
print(paste("Test for stationarity could not reject H0:",
"Time series has a unit root.",
"Consider detrending your time series.",
sep = " "))
q <- readline("Would you like to proceed anyway? (Y/N)")
if(regexpr(q, 'y', ignore.case = TRUE) == 1){
next
} else {
break
}
}
### Calculate rolling window statistics for ts ###
# Set maximium window size
max.ws = floor(length(ts)/2)
# Lag-1-autocorrelation
acf  = c(1:length(ts))
for(i in 3:max.ws) {
temp = getMovingWindowACF(ts, w = i)
acf  = data.frame(acf, c(rep(NA, i-1), temp))
colnames(acf)[ncol(acf)] = "acf_ws"%&%i
}
# Variance
var  = c(1:length(ts))
for(i in 3:max.ws) {
temp = getMovingWindowVAR(ts, w = i)
var  = data.frame(var, c(rep(NA, i-1), temp))
colnames(var)[ncol(var)] = "var_ws"%&%i
}
# Skewness
skew = c(1:length(ts))
for(i in 3:max.ws) {
temp = getMovingWindowSKEW(ts, w = i)
skew = data.frame(skew, c(rep(NA, i-1), temp))
colnames(skew)[ncol(skew)] = "skew_ws"%&%i
}
# W_2 measure
W2 = c(1:length(ts))
for(ws in 3:max.ws) {
n    = c(rep(NA, (ws-1)), 1:(length(ts)-ws+1))
temp =   (acf[1:nrow(acf), 2] -
c(rep(NA, ws-1), cumsum(acf[ws:nrow(acf) , 2])) / n) /
getRunningSD(acf, ws)
+ (var[1:nrow(var), 2] -
c(rep(NA, ws-1), cumsum(var[ws:nrow(var) , 2])) / n) /
getRunningSD(var, ws)
+ (skew[1:nrow(skew), 2] -
c(rep(NA, ws-1), cumsum(skew[ws:nrow(skew) , 2])) / n) /
getRunningSD(skew, ws)
W2   = data.frame(W2, temp)
colnames(W2)[ncol(W2)] = "W2_ws"%&%ws
}
# Create data frame containing running mean/standard deviation
# of W2 measure to identify CSD areas
flag = flagCSD(W2)
# Write out data frames
#for(i in c("acf", "var", "skew", "flag", "W2", "ts")){
#    write.csv(i, ""%&%sentiment%&%"_"%&%i%&%".csv")
#}
# Tidy up
rm(max.ws, n, temp, ws, i)
}
# end of file
y
# Get data
ts =  getData("tweets_raw.csv", sentiment)[[2]]
# Check for missing values
if(any(is.na(ts))) {
stop(paste("There are missing values present in this time series.",
"Interpolate raw data first.", sep = " "))
}
# Test for stationarity
if(adf.test(ts)$p.value > 0.05) {
print(paste("Test for stationarity could not reject H0:",
"Time series has a unit root.",
"Consider detrending your time series.",
sep = " "))
q <- readline("Would you like to proceed anyway? (Y/N)")
if(regexpr(q, 'y', ignore.case = TRUE) == 1){
next
} else {
break
}
}
adf.test(ts)$p.value
sentiment <- "positive"
# Get data
ts =  getData("tweets_raw.csv", sentiment)[[2]]
# Check for missing values
if(any(is.na(ts))) {
stop(paste("There are missing values present in this time series.",
"Interpolate raw data first.", sep = " "))
}
# Test for stationarity
if(adf.test(ts)$p.value > 0.05) {
print(paste("Test for stationarity could not reject H0:",
"Time series has a unit root.",
"Consider detrending your time series.",
sep = " "))
q <- readline("Would you like to proceed anyway? (Y/N)")
if(regexpr(q, 'y', ignore.case = TRUE) == 1){
next
} else {
break
}
}
adf.test(ts)$p.value
rm(list = ls())
# Load required packages
if(!require("tseries")) install.packages("tseries")
library("tseries")
# Source custom functions
function.sources = list.files(pattern = "(TSAfun_){1}.*.R")
sapply(function.sources, source, .GlobalEnv)
# Function for easy string pasting
"%&%" = function(x, y) paste(x, y, sep = "")
# Repeat whole analysis for positive as well as negative sentiment share
for(sentiment in c("positive", "negative")){
# Get data
ts =  getData("tweets_raw.csv", sentiment)[[2]]
# Check for missing values
if(any(is.na(ts))) {
stop(paste("There are missing values present in this time series.",
"Interpolate raw data first.", sep = " "))
}
# Test for stationarity
if(adf.test(ts)$p.value > 0.05) {
print(paste("Test for stationarity could not reject H0:",
"Time series has a unit root.",
"Consider detrending your time series.",
sep = " "))
q <- readline("Would you like to proceed anyway? (Y/N)")
if(regexpr(q, 'y', ignore.case = TRUE) == 1){
next
} else {
break
}
}
### Calculate rolling window statistics for ts ###
# Set maximium window size
max.ws = floor(length(ts)/2)
# Lag-1-autocorrelation
acf  = c(1:length(ts))
for(i in 3:max.ws) {
temp = getMovingWindowACF(ts, w = i)
acf  = data.frame(acf, c(rep(NA, i-1), temp))
colnames(acf)[ncol(acf)] = "acf_ws"%&%i
}
# Variance
var  = c(1:length(ts))
for(i in 3:max.ws) {
temp = getMovingWindowVAR(ts, w = i)
var  = data.frame(var, c(rep(NA, i-1), temp))
colnames(var)[ncol(var)] = "var_ws"%&%i
}
# Skewness
skew = c(1:length(ts))
for(i in 3:max.ws) {
temp = getMovingWindowSKEW(ts, w = i)
skew = data.frame(skew, c(rep(NA, i-1), temp))
colnames(skew)[ncol(skew)] = "skew_ws"%&%i
}
# W_2 measure
W2 = c(1:length(ts))
for(ws in 3:max.ws) {
n    = c(rep(NA, (ws-1)), 1:(length(ts)-ws+1))
temp =   (acf[1:nrow(acf), 2] -
c(rep(NA, ws-1), cumsum(acf[ws:nrow(acf) , 2])) / n) /
getRunningSD(acf, ws)
+ (var[1:nrow(var), 2] -
c(rep(NA, ws-1), cumsum(var[ws:nrow(var) , 2])) / n) /
getRunningSD(var, ws)
+ (skew[1:nrow(skew), 2] -
c(rep(NA, ws-1), cumsum(skew[ws:nrow(skew) , 2])) / n) /
getRunningSD(skew, ws)
W2   = data.frame(W2, temp)
colnames(W2)[ncol(W2)] = "W2_ws"%&%ws
}
# Create data frame containing running mean/standard deviation
# of W2 measure to identify CSD areas
flag = flagCSD(W2)
# Write out data frames
#for(i in c("acf", "var", "skew", "flag", "W2", "ts")){
#    write.csv(i, ""%&%sentiment%&%"_"%&%i%&%".csv")
#}
# Tidy up
rm(max.ws, n, temp, ws, i)
}
# end of file
rm(list = ls())
# Load required packages
if(!require("tseries")) install.packages("tseries")
library("tseries")
# Source custom functions
function.sources = list.files(pattern = "(TSAfun_){1}.*.R")
sapply(function.sources, source, .GlobalEnv)
# Function for easy string pasting
"%&%" = function(x, y) paste(x, y, sep = "")
# Repeat whole analysis for positive as well as negative sentiment share
for(sentiment in c("positive", "negative")){
# Get data
ts =  getData("tweets_raw.csv", sentiment)[[2]]
# Check for missing values
if(any(is.na(ts))) {
stop(paste("There are missing values present in this time series.",
"Interpolate raw data first.", sep = " "))
}
# Test for stationarity
if(adf.test(ts)$p.value > 0.05) {
print(paste("Test for stationarity could not reject H0:",
"Time series has a unit root.",
"Consider detrending your time series.",
sep = " "))
q <- readline("Would you like to proceed anyway? (Y/N)")
if(regexpr(q, 'y', ignore.case = TRUE) == 1){
next
} else {
break
}
}
### Calculate rolling window statistics for ts ###
# Set maximium window size
max.ws = floor(length(ts)/2)
# Lag-1-autocorrelation
acf  = c(1:length(ts))
for(i in 3:max.ws) {
temp = getMovingWindowACF(ts, w = i)
acf  = data.frame(acf, c(rep(NA, i-1), temp))
colnames(acf)[ncol(acf)] = "acf_ws"%&%i
}
# Variance
var  = c(1:length(ts))
for(i in 3:max.ws) {
temp = getMovingWindowVAR(ts, w = i)
var  = data.frame(var, c(rep(NA, i-1), temp))
colnames(var)[ncol(var)] = "var_ws"%&%i
}
# Skewness
skew = c(1:length(ts))
for(i in 3:max.ws) {
temp = getMovingWindowSKEW(ts, w = i)
skew = data.frame(skew, c(rep(NA, i-1), temp))
colnames(skew)[ncol(skew)] = "skew_ws"%&%i
}
# W_2 measure
W2 = c(1:length(ts))
for(ws in 3:max.ws) {
n    = c(rep(NA, (ws-1)), 1:(length(ts)-ws+1))
temp =   (acf[1:nrow(acf), 2] -
c(rep(NA, ws-1), cumsum(acf[ws:nrow(acf) , 2])) / n) /
getRunningSD(acf, ws)
+ (var[1:nrow(var), 2] -
c(rep(NA, ws-1), cumsum(var[ws:nrow(var) , 2])) / n) /
getRunningSD(var, ws)
+ (skew[1:nrow(skew), 2] -
c(rep(NA, ws-1), cumsum(skew[ws:nrow(skew) , 2])) / n) /
getRunningSD(skew, ws)
W2   = data.frame(W2, temp)
colnames(W2)[ncol(W2)] = "W2_ws"%&%ws
}
# Create data frame containing running mean/standard deviation
# of W2 measure to identify CSD areas
flag = flagCSD(W2)
# Write out data frames
#for(i in c("acf", "var", "skew", "flag", "W2", "ts")){
#    write.csv(i, ""%&%sentiment%&%"_"%&%i%&%".csv")
#}
# Tidy up
rm(max.ws, n, temp, ws, i)
}
# end of file
n
rm(list = ls())
# Load required packages
if(!require("tseries")) install.packages("tseries")
library("tseries")
# Source custom functions
function.sources = list.files(pattern = "(TSAfun_){1}.*.R")
sapply(function.sources, source, .GlobalEnv)
rm(list = ls())
# Source custom functions
function.sources = list.files(pattern = "(TSAfun_){1}.*.R")
sapply(function.sources, source, .GlobalEnv)
# Function for easy string pasting
"%&%" = function(x, y) paste(x, y, sep = "")
# Get data
ts = getData("tweets_raw.csv")[[2]]
#### Calculate rolling window statistics for ts ####
# Set maximium window size
max.ws = floor(length(ts)/2)
# Lag-1-autocorrelation
acf = c(1:length(ts))
for(i in 3:max.ws) {
temp = getMovingWindowACF(ts, w = i)
acf  = data.frame(acf, c(rep(NA, i-1), temp))
colnames(acf)[ncol(acf)] = "acf_ws"%&%i
}
# Variance
var = c(1:length(ts))
for(i in 3:max.ws) {
temp = getMovingWindowVAR(ts, w = i)
var  = data.frame(var, c(rep(NA, i-1), temp))
colnames(var)[ncol(var)] = "var_ws"%&%i
}
# Skewness
skew = c(1:length(ts))
for(i in 3:max.ws) {
temp = getMovingWindowSKEW(ts, w = i)
skew  = data.frame(skew, c(rep(NA, i-1), temp))
colnames(skew)[ncol(skew)] = "skew_ws"%&%i
}
#### Test getRunningSD function ####
# Test whether getRunningSD function and for-loop calculating standard
# deviation with standard package functions generate same results
# Set tolerance for deviations between results (Smaller deviations
# are assumed to be due to accumulated rounding differences)
tol = 1e-15
# Iterate through window sizes
for(ws in 3:floor(length(ts)/2)) {
# Lag-1-autocorrelation
run_sd_acf  = getRunningSD(acf, ws)
run_sd_acf2 = rep(NA, nrow(acf))
for (i in (ws+1):nrow(acf)) {
run_sd_acf2[i] = sd(acf[ws:i, ws-1])
}
print(paste("Window size "%&%ws%&%":"))
print(paste("Lag-1-autocorrelation ->",
if(max(abs(run_sd_acf-run_sd_acf2), na.rm = TRUE) > tol) {
paste("Maximum absolute deviation between getRunningSD",
"and for-loop is: "%&%max(abs(run_sd_acf-run_sd_acf2),
na.rm = TRUE))
} else {
paste("All deviations are smaller than "%&%tol)
}
))
# Variance
run_sd_var  = getRunningSD(var, ws)
run_sd_var2 = rep(NA, nrow(var))
for (i in (ws+1):nrow(var)) {
run_sd_var2[i] = sd(var[ws:i, ws-1])
}
print(paste("Variance              ->",
if(max(abs(run_sd_var-run_sd_var2), na.rm = TRUE) > tol) {
paste("Maximum absolute deviation between getRunningSD",
"and for-loop is: "%&%max(abs(run_sd_var-run_sd_var2),
na.rm = TRUE))
} else {
paste("All deviations are smaller than "%&%tol)
}
))
# Skewness
run_sd_skew  = getRunningSD(skew, ws)
run_sd_skew2 = rep(NA, nrow(skew))
for (i in (ws+1):nrow(skew)) {
run_sd_skew2[i] = sd(skew[ws:i, ws-1])
}
print(paste("Skewness              ->",
if(max(abs(run_sd_skew-run_sd_skew2), na.rm = TRUE) > tol) {
paste("Maximum absolute deviation between getRunningSD",
"and for-loop is: "%&%max(abs(run_sd_skew-run_sd_skew2),
na.rm = TRUE))
} else {
paste("All deviations are smaller than "%&%tol)
}
))
# Tidy up after each window size
rm(ws, list = ls(pattern = "run_"))
}
